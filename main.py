import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from tokenizers import Tokenizer
from safetensors.torch import load_model
import numpy as np

# Û±. Ø¯ Ù…Ø§Ú‰Ù„ Ø§Ø±Ú©ÛŒÙ¼ÛŒÚ©Ú†Ø± (Architecture)
class GhanamModel(torch.nn.Module):
    def __init__(self, vocab_size=65536, n_embd=768, n_head=12, n_layer=12):
        super().__init__()
        self.embeddings = torch.nn.Embedding(vocab_size, n_embd)
        self.layers = torch.nn.ModuleList([
            torch.nn.TransformerEncoderLayer(
                d_model=n_embd, nhead=n_head, dim_feedforward=3072, 
                batch_first=True, activation='gelu'
            ) for _ in range(n_layer)
        ])
        self.norm_final = torch.nn.LayerNorm(n_embd)
        self.lm_head = torch.nn.Linear(n_embd, vocab_size, bias=False)

    def forward(self, idx):
        x = self.embeddings(idx)
        for layer in self.layers:
            x = layer(x)
        return self.lm_head(self.norm_final(x))

# Û². Ø¯ Backend ØªÙ†Ø¸ÛŒÙ…Ø§Øª
app = FastAPI(title="Ghanam-1B Pashto AI Engine")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ø¯ Ù¼ÙˆÚ©Ù†Ø± Ø§Ùˆ Ù…Ø§Ú‰Ù„ Ù„ÙˆÚ‰ Ú©ÙˆÙ„
try:
    tokenizer = Tokenizer.from_file("Ghanam-1B-Tokenizer_Fixed.json")
    model = GhanamModel()
    load_model(model, "model.safetensors")
    model.to(device)
    model.eval()
    print(f"âœ… Ghanam-1B is LIVE on {device}")
except Exception as e:
    print(f"âŒ Initialization Error: {e}")

# Û³. Ø¯ ØºÙˆÚšØªÙ†Û Ø³Ù¼Ø±Ø§Ú©Ú†Ø± (Input Validation)
class GhanamRequest(BaseModel):
    prompt: str
    max_len: int = 64
    temperature: float = 0.75
    top_p: float = 0.92
    top_k: int = 50
    repetition_penalty: float = 1.8  # Ø¯ ØªÚ©Ø±Ø§Ø± Ù…Ø®Ù†ÛŒÙˆÙŠ Ù„Ù¾Ø§Ø±Ù‡ Ø¬Ø±ÛŒÙ…Ù‡

# Û´. Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø³Ù…Ø§Ø±Ù¼ Ù…Ù†Ø·Ù‚ (Generation Logic)
def generate_response(req: GhanamRequest):
    input_ids = torch.tensor([tokenizer.encode(req.prompt).ids]).to(device)
    generated = input_ids
    
    for _ in range(req.max_len):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs[:, -1, :] / req.temperature
            
            # ğŸ”¥ Repetition Penalty Logic
            # Ù‡ØºÙ‡ Ù¼ÙˆÚ©Ù†ÙˆÙ†Ù‡ Ú†Û Ù„Ø§ Ø¯Ù…Ø®Ù‡ Ú©Ø§Ø±ÙˆÙ„ Ø´ÙˆÙŠØŒ Ú†Ø§Ù†Ø³ ÛŒÛ Ú©Ù…ÙˆÙŠ
            for token_id in set(generated[0].tolist()):
                if token_id == tokenizer.token_to_id(" Ø¯"):
                    logits[0, token_id] /= (req.repetition_penalty + 1.0) # Ø¯ "Ø¯" Ù„Ù¾Ø§Ø±Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø¬Ø±ÛŒÙ…Ù‡
                else:
                    logits[0, token_id] /= req.repetition_penalty

            # Top-K Sampling
            top_k_logits, _ = torch.topk(logits, min(req.top_k, logits.size(-1)))
            logits[logits < top_k_logits[:, [-1]]] = -float('Inf')
            
            # Top-P (Nucleus) Sampling
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > req.top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            logits[0, indices_to_remove] = -float('Inf')
            
            # Ø¯ Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÙˆ Ú…Ø®Ù‡ Ø§Ù†ØªØ®Ø§Ø¨
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            generated = torch.cat((generated, next_token), dim=1)
            
            # Ú©Ù‡ Ø¯ Ø¬Ù…Ù„Û Ù¾Ø§ÛŒ (End of Sentence) Ø±Ø§ØºÛŒ
            if next_token.item() == tokenizer.token_to_id("</s>"):
                break
                
    return tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)

# Ûµ. Ø¯ API Ø¨Ø±Ø®Û (Endpoints)
@app.get("/")
def home():
    return {"message": "Ghanam-1B Backend is running!", "status": "online"}

@app.post("/chat")
async def chat(request: GhanamRequest):
    if not request.prompt.strip():
        raise HTTPException(status_code=400, detail="Ù¾Ø±Ø§Ù…Ù¾Ù¼ Ø®Ø§Ù„ÙŠ Ø¯ÛŒ!")
    
    try:
        raw_output = generate_response(request)
        # Ø¯ Ú©Ø§Ø±ÛØ¬ Ø±ÛŒÙ¼Ø±Ù† (\r) Ø§Ùˆ Ø§Ø¶Ø§ÙÙŠ Ø³Ù¾ÛØ³ÙˆÙ†Ùˆ Ù¾Ø§Ú©ÙˆÙ„
        clean_output = raw_output.replace('\r', '').replace('\n', ' ').strip()
        
        return {
            "prompt": request.prompt,
            "response": clean_output,
            "model": "Ghanam-1B-v0.1"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Û¶. Ø¯ Ø³Ø±ÙˆØ± Ú†Ø§Ù„Ø§Ù†ÙˆÙ„
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)