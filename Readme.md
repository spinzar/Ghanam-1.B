# ðŸŒ¾ Ghanam-1.B
![Alt text for the image](images/design.png)

**Ghanam-1.B** is a native **Pashto Base Language Model** built *from scratch* for education, community use, and longâ€‘term AI infrastructure in the Pashto language.  
The project is designed with a **Pashtoâ€‘first philosophy**, avoiding Englishâ€‘centric assumptions at every layer: data, tokenizer, and architecture.

---

## ðŸ“Œ Vision

- Build a **foundational Pashto language model** without relying on pretrained English weights
- Support **children, students, and general Pashto users**
- Enable **community fineâ€‘tuning** without dependency on hyperscalers
- Treat Pashto as a *firstâ€‘class language*, not a downstream adaptation

---

## ðŸ§  Model Overview

| Property | Value |
|--------|------|
| Model Name | **Ghanam-1.B** |
| Parameters | ~1 Billion |
| Architecture | **Liquid Foundation Model (Hybrid)** |
| Training | From scratch |
| License | Open (Communityâ€‘oriented) |

---

## ðŸ§¬ Architecture: Liquid Foundation Model (LFM)

Ghanam-1.B uses a **hybrid architecture** combining:

- **Localized Convolutions** â†’ fast processing of local syntax and morphology
- **Selfâ€‘Attention** â†’ longâ€‘range semantic understanding

Typical layer pattern:

```
[Conv] â†’ [Attention] â†’ [Conv]
```

### Why LFM for Pashto?

- Lower compute cost than Transformerâ€‘only models
- Better handling of rich morphology
- Suitable for **lowâ€‘resource languages**
- Efficient on small GPU clusters

---

## âš™ï¸ Model Scale

- **Layers:** 24â€“28 (Hybrid)
- **Hidden Size:** 2048â€“2560
- **Attention Heads:** 16â€“20
- **Context Length:** 4kâ€“8k tokens
- **Precision:** bfloat16

The scale is intentionally chosen to balance **expressive power** and **practical deployability**.

---

## ðŸ”¤ Tokenizer (Pashtoâ€‘Native)

Ghanam-1.B does **not** use any Englishâ€‘centric tokenizer.

### Tokenizer Specs

- **Vocabulary Size:** 65,536
- **Training Data:** 100% Pashto
- **Unicodeâ€‘aware**
- **Morphologyâ€‘friendly**

### Pashto Linguistic Core

The tokenizer is seeded with Pashtoâ€‘specific characters:

```
Ù¼ Ú‰ Ú“ Ú– Úš Ú… Ú Ú¼ Û Û Ú«
```

It also learns frequent Pashto morphemes such as:

- Ù€ÙˆÙ†Ù‡
- Ù€ÙˆØ§Ù„
- Ù€ÛŒØ²
- Ù€Ú«Ø±
- Ù€ÙˆØ§Ù„ÛŒ

This ensures efficient tokenization without semantic fragmentation.

---

## ðŸ“š Data Policy

### Included

- Native Pashto texts
- Educational and childâ€‘safe content
- Narrative and conversational Pashto
- Scientific and encyclopedic Pashto (localized)

### Filtered / Limited

- Englishâ€‘dominant text
- Roman Pashto
- Urdu slang
- Unadapted Arabic/Farsi constructions

> Pashtoâ€‘first does **not** mean antiâ€‘loanwords; it means **Pashtoâ€‘dominant**.

---

## ðŸ›¡ï¸ Safety by Design

- Profanity filtering at tokenizer level
- Toxic pattern suppression
- No early RLHF (to avoid bias injection)
- Instruction tuning applied *after* base training

Designed to be suitable for **children and educational use**.

---

## ðŸŒ Community & Openness

- Trainable and runnable on **small clusters**
- Fineâ€‘tuneâ€‘friendly for researchers and educators
- No dependency on closed APIs or hyperscalers

The goal is a **communityâ€‘owned Pashto AI foundation**.

---

## ðŸŒ¾ Why the Name "Ghanam"?

**Ghanam (ØºÙ†Ù…)** means *wheat* â€” the basic food.

Just as wheat forms the base of nourishment, **Ghanamâ€‘1.B** is designed to be the **base nourishment for Pashto AI systems**.

---

## ðŸš€ Roadmap

- [ ] Tokenizer training release
- [ ] Full architecture blueprint (PyTorch)
- [ ] Training logs & metrics
- [ ] Instructionâ€‘tuned variants
- [ ] Educational fineâ€‘tunes

---

## ðŸ¤ Contribution

Contributions are welcome from:

- Pashto linguists
- ML researchers
- Educators
- Openâ€‘source contributors

This project grows **with the community**.

---

## ðŸ“œ Status

**Ghanamâ€‘1.B** is under active design and development.

> From Pashto. For Pashto. Built to last.

